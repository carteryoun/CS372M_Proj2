{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780305ee-d6f9-47ec-8501-d02d3f0050d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T06:01:51.172787Z",
     "start_time": "2024-03-05T06:01:47.074994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data loader: 1 batches of 256\n",
      "Length of validation data loader: 1 batches of 256\n",
      "Length of test data loader: 1 batches of 256\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 65\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLength of test data loader: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(test_loader)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m batches of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# Check out what is inside the training data loader\u001B[39;00m\n\u001B[0;32m---> 65\u001B[0m train_features_batch, train_label_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(train_loader))\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_features_batch\u001B[38;5;241m.\u001B[39mshape, train_label_batch\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Do not change this cell\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "trainset = 'project3/cleaned_train_data.csv'\n",
    "\n",
    "validationset = 'project3/cleaned_validation_data.csv'\n",
    "\n",
    "testset = 'project3/cleaned_test_data.csv'\n",
    "\n",
    "classes = len(trainset)\n",
    "\n",
    "\n",
    "valid_size = 0.2\n",
    "train_length = len(trainset)\n",
    "indices = list(range(len(trainset)))\n",
    "split = int(np.floor(valid_size * train_length))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx=indices[split:]\n",
    "valid_idx=indices[:split]\n",
    "train_sampler=SubsetRandomSampler(train_idx)\n",
    "validation_sampler=SubsetRandomSampler(valid_idx)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
    "valid_loader = DataLoader(trainset, batch_size=batch_size, sampler=validation_sampler)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Length of train data loader: {len(train_loader)} batches of {batch_size}\")\n",
    "print(f\"Length of validation data loader: {len(valid_loader)} batches of {batch_size}\")\n",
    "print(f\"Length of test data loader: {len(test_loader)} batches of {batch_size}\")\n",
    "\n",
    "# Check out what is inside the training data loader\n",
    "train_features_batch, train_label_batch = next(iter(train_loader))\n",
    "print(train_features_batch.shape, train_label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1f4b4-5c23-4e84-af74-fbf3f7404ba1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T06:01:51.172398Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f1c46-9ce7-4aa4-ae58-5234cfe6ca48",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T06:01:51.173554Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b02119-0bc1-44e9-b7e7-b71a8cd3b4a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T06:01:51.175590Z",
     "start_time": "2024-03-05T06:01:51.174466Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: you will design your model here\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ConvModel, self).__init__()\n",
    "        # Define arch of CNN\n",
    "        # Size of input (3, RGB), output (# filters), kernel (5x5), stride and padding\n",
    "        self.conv1 = nn.Conv2d(input_size, 32, kernel_size=5, stride=1, padding=2)\n",
    "        # Reduces spatial dimensions using 2x2 filter and stride 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # Takes 32 channels input from conv1 and produces 64 channels\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "        # Size after first pool: (28/2)=14\n",
    "        # Size after second: (14/2)=7\n",
    "        \n",
    "        # Two fully connected linear layers\n",
    "        # fc1 takes flattened object and produces 128 features\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        # fc2 takes 128 input and produces the final output\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten tensor\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17772d-49e7-4d06-a14d-aff4bc9715a1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T06:01:51.175367Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(model, train_loader, loss_fn, optimizer, reg_param, device):\n",
    "    # Initialize model\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear the gradients of all optimized vars\n",
    "        output = model(data)  # Forward pass: compute predicted outputs by passing to the model\n",
    "        loss = loss_fn(output, target)  # This is our prediction loss\n",
    "        \n",
    "        # L2 Regularization and loss calculation\n",
    "        l2_reg = torch.tensor(0.).to(device)\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "        loss += reg_param * l2_reg  # Correct loss equation\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 1 optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "    # Calculate avg loss\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Return\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9baf8-6989-4789-9fb4-ecccaf1f2139",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T06:01:51.176492Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation_step(model, data_loader, loss_fn, reg_param, device):\n",
    "    # Initialize model\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    total_samples = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # Do not track gradients\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move data to device\n",
    "            \n",
    "            output = model(X_batch)  # Forward pass\n",
    "            loss = loss_fn(output, y_batch)  # Compute batch loss\n",
    "            \n",
    "            if reg_param is not None:  # Apply L2 reg if reg_param > 0\n",
    "                l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                loss += reg_param * l2_reg  # Correct loss eqn\n",
    "                \n",
    "            total_loss += loss.item() * X_batch.size(0)  # Aggregate batch loss\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)  # Get predictions\n",
    "            correct_predictions += (predicted == y_batch).sum().item()  # Count correct preds\n",
    "            total_samples += y_batch.size(0)  # Count total samples\n",
    "            \n",
    "        avg_loss = total_loss / total_samples  # Calc avg loss\n",
    "        accuracy = correct_predictions / total_samples * 100\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecae0d8-d040-4e9a-8b3f-f64f97e988a2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T06:01:51.177713Z"
    }
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, initial_lr=0.45, decay_rate=0.99):\n",
    "    \"\"\" Adjusts learning rate based on epoch number \"\"\"\n",
    "    lr = initial_lr * (decay_rate ** epoch)\n",
    "    for param_group in optimizer.parameters:\n",
    "        param_group.lr = lr\n",
    "\n",
    "class SimpleGradDescentOptimizer:\n",
    "    \"\"\" This is a simple optimizer which provides functions for \n",
    "        optimization steps and zeroing out the gradient. \"\"\"\n",
    "    def __init__(self, parameters, lr=0.45):\n",
    "        self.parameters = list(parameters)\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\" Perform a single optimization step \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters:\n",
    "                if param.grad is not None:\n",
    "                    param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\" Clear gradients of all optimized parameters \"\"\"\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "                \n",
    "def train_conv_model(train_loader, valid_loader, test_loader, random_seed):\n",
    "    torch.manual_seed(random_seed)  # do not change this\n",
    "    \n",
    "    # Make device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    #else:\n",
    "        #if torch.backends.mps.is_available():\n",
    "            #device = \"mps\"\n",
    "    \n",
    "    # Model initialization\n",
    "    input_size = 1 \n",
    "    output_size = len(classes)  # Output classes\n",
    "    model = ConvModel(input_size, output_size).to(device)\n",
    "    \n",
    "    # Loss fn\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = SimpleGradDescentOptimizer(model.parameters(), lr=0.45)\n",
    "    \n",
    "    # Regularization p\n",
    "    reg_param = 0.001\n",
    "    \n",
    "    # Trackers\n",
    "    train_losses, train_accuracies = [], []\n",
    "    valid_losses, valid_accuracies = [], []\n",
    "    test_losses, test_accuracies = [], []\n",
    "    \n",
    "    num_epochs = 25\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = timer()\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # Training step\n",
    "        train_loss, train_accuracy = train_step(model, train_loader, loss_fn, optimizer, reg_param, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation step\n",
    "        valid_loss, valid_accuracy = evaluation_step(model, valid_loader, loss_fn, reg_param, device)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "        \n",
    "        end_time = timer()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Valid Loss: {valid_loss:.4f}, \"\n",
    "              f\"Valid Accuracy: {valid_accuracy:.2f}%, \"\n",
    "              f\"Time: {end_time - start_time:.2f}s\")\n",
    "        \n",
    "        # Test step\n",
    "        test_loss, test_accuracy = evaluation_step(model, test_loader, loss_fn, reg_param, device)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    return model, train_losses, train_accuracies, valid_losses, valid_accuracies, test_losses, test_accuracies\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d29a37-67eb-44e2-971c-66b7817923ea",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T06:01:51.178807Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_performance(train_accuracies, valid_accuracies, test_accuracies):\n",
    "    plt.figure(figsize=(10, 6))  # make plt object\n",
    "    epochs = range(1, len(train_accuracies) + 1)  # size of x\n",
    "    \n",
    "    # Plot training and test acc, as well as validation\n",
    "    plt.plot(epochs, train_accuracies, 'bo-', label='Training Accuracy')  # y-axis for train\n",
    "    plt.plot(epochs, test_accuracies, 'ro-', label='Test Accuracy')  # y-axis for test\n",
    "    plt.plot(epochs, valid_accuracies, 'go-', label='Validation Accuracy')\n",
    "    \n",
    "    \n",
    "    # Title and Labels\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    # Legend\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce449fa4-1519-40cc-a7d2-c36b3cffa068",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T06:01:51.179460Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss_performance(train_losses, valid_accuracies, test_losses):\n",
    "    plt.figure(figsize=(10, 6))  # make plt object\n",
    "    epochs = range(1, len(train_accuracies) + 1)  # size of x\n",
    "    \n",
    "    # Plot training and test losses, as well as validation\n",
    "    plt.plot(epochs, train_losses, 'bo-', label='Training Loss')  # y-axis for train\n",
    "    plt.plot(epochs, test_losses, 'ro-', label='Test Loss')  # y-axis for test\n",
    "    plt.plot(epochs, valid_accuracies, 'go-', label='Validation Loss')  # y-axis for validation\n",
    "    \n",
    "    # Title and Labels\n",
    "    plt.title('Training and Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    # Legend\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd20d6d-0bb1-48fc-8e9c-809f8a7b6c62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T06:01:51.181309Z",
     "start_time": "2024-03-05T06:01:51.180139Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not change this cell\n",
    "random_seed = 1\n",
    "model, train_losses, train_accuracies, valid_losses, valid_accuracies, test_losses, test_accuracies\\\n",
    "= train_conv_model(train_loader, valid_loader, test_loader, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ffc77-8d95-4676-9ba0-4fa083b235ed",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T06:01:51.180753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not change this cell\n",
    "plot_accuracy_performance(train_accuracies, valid_accuracies, test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408016e-99e3-4a8a-add3-c3e7a29d5162",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-05T06:01:51.184126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not change this cell\n",
    "plot_loss_performance(train_losses, valid_losses, test_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
